{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":99552,"databundleVersionId":13851420,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport re\nimport glob\nimport time\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import convert_color_space\nfrom tqdm import tqdm\nfrom joblib import Parallel, delayed\nfrom multiprocessing import Pool, cpu_count\nimport warnings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T19:35:48.849268Z","iopub.execute_input":"2025-11-13T19:35:48.849578Z","iopub.status.idle":"2025-11-13T19:35:52.435221Z","shell.execute_reply.started":"2025-11-13T19:35:48.849555Z","shell.execute_reply":"2025-11-13T19:35:52.433962Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# ==========================================\n# 1. INSTALL & CONFIG\n# ==========================================\n# We use standard pip installs to ensure it runs on any fresh notebook\nos.system('pip install -q dicomsdl python-gdcm pydicom pylibjpeg pylibjpeg-libjpeg pylibjpeg-openjpeg')\n\nDEBUG = False  # Set to True for a quick test, False for full dataset\nGLOBAL_WIDTH = 224\nRD = '/kaggle/input/rsna-intracranial-aneurysm-detection'\nOUTPUT_DIR = '/kaggle/working/cvt_png'\n\n# Create output directory\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\nprint(f\"Debug Mode: {DEBUG}\")\nprint(f\"Global Width: {GLOBAL_WIDTH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T19:35:52.437207Z","iopub.execute_input":"2025-11-13T19:35:52.437711Z","iopub.status.idle":"2025-11-13T19:36:05.611712Z","shell.execute_reply.started":"2025-11-13T19:35:52.437685Z","shell.execute_reply":"2025-11-13T19:36:05.610775Z"}},"outputs":[{"name":"stdout","text":"     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.1/62.1 kB 1.5 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 18.7 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.3/13.3 MB 76.4 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.4/3.4 MB 64.4 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 42.6 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.9/16.9 MB 73.7 MB/s eta 0:00:00\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.4 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.4 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.4 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.4 which is incompatible.\ndatasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nydata-profiling 4.17.0 requires numpy<2.2,>=1.16.0, but you have numpy 2.3.4 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.4 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.4 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\n","output_type":"stream"},{"name":"stdout","text":"Debug Mode: False\nGlobal Width: 224\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ==========================================\n# 2. HELPER FUNCTIONS (Sorting & Windowing)\n# ==========================================\ndef get_windowing_params(modality):\n    windows = {\n        'CT': (40, 80),\n        'CTA': (50, 350),\n        'MRA': (600, 1200),\n        'MRI': (40, 80),\n    }\n    return windows.get(modality, (40, 80))\n\ndef apply_dicom_windowing(img, window_center, window_width):\n    img_min = window_center - window_width // 2\n    img_max = window_center + window_width // 2\n    img = np.clip(img, img_min, img_max)\n    img = (img - img_min) / (img_max - img_min + 1e-7)\n    return (img * 255).astype(np.uint8)\n\ndef extract_sort_key(path):\n    try:\n        ds = pydicom.dcmread(path, stop_before_pixels=True, force=True)\n        instance_number = getattr(ds, 'InstanceNumber', None)\n        position = getattr(ds, 'ImagePositionPatient', [None, None, None])\n        z = position[2] if position and len(position) == 3 else None\n        \n        if instance_number is not None:\n            return (int(instance_number), 0, path)\n        elif z is not None:\n            return (float('inf'), float(z), path)\n        else:\n            return (float('inf'), float('inf'), path)\n    except:\n        return (float('inf'), float('inf'), path)\n\ndef sort_series(args):\n    series_uid, paths = args\n    # Sort paths based on InstanceNumber or Z-position\n    sort_info = [extract_sort_key(p) for p in paths]\n    sort_info.sort()\n    return series_uid, [x[2] for x in sort_info]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T19:36:05.612667Z","iopub.execute_input":"2025-11-13T19:36:05.613082Z","iopub.status.idle":"2025-11-13T19:36:05.623678Z","shell.execute_reply.started":"2025-11-13T19:36:05.613057Z","shell.execute_reply":"2025-11-13T19:36:05.622322Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# ==========================================\n# 3. SORTING DICOMS & MAPPING\n# ==========================================\nprint(\">>> Step 1: Loading and Sorting Data...\")\ndf_train = pd.read_csv(f'{RD}/train.csv')\n\nif DEBUG:\n    series_uids = df_train['SeriesInstanceUID'].unique()[:5]\n    df_train = df_train[df_train['SeriesInstanceUID'].isin(series_uids)]\nelse:\n    series_uids = df_train['SeriesInstanceUID'].unique()\n\n# Map SeriesUID -> List of Paths\nseries_dicom_map = {\n    si: glob.glob(os.path.join(RD, 'series', si, '*.dcm'))\n    for si in series_uids\n}\n\n# Parallel Sort\nwith Pool(cpu_count()) as pool:\n    sorted_results = list(tqdm(pool.imap(sort_series, series_dicom_map.items()),\n                               total=len(series_dicom_map),\n                               desc=\"Sorting Series\"))\n\n# Generate Mapping CSV\nrows = []\nfor series_uid, sorted_paths in sorted_results:\n    try:\n        modality = df_train[df_train['SeriesInstanceUID'] == series_uid]['Modality'].iloc[0]\n    except:\n        modality = 'CT'\n        \n    for idx, path in enumerate(sorted_paths):\n        sop_uid = os.path.splitext(os.path.basename(path))[0]\n        rows.append({\n            'SeriesInstanceUID': series_uid,\n            'SOPInstanceUID': sop_uid,\n            'dicom_filename': path,\n            'relative_index': idx,\n            'Modality': modality\n        })\n\ndf_series_index_mapping = pd.DataFrame(rows)\ndf_series_index_mapping.sort_values(by=['SeriesInstanceUID', 'relative_index'], inplace=True)\ndf_series_index_mapping.to_csv('series_index_mapping.csv', index=False)\nprint(\"Saved series_index_mapping.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T19:36:05.625600Z","iopub.execute_input":"2025-11-13T19:36:05.626095Z","iopub.status.idle":"2025-11-13T20:31:38.244696Z","shell.execute_reply.started":"2025-11-13T19:36:05.626066Z","shell.execute_reply":"2025-11-13T20:31:38.242710Z"}},"outputs":[{"name":"stdout","text":">>> Step 1: Loading and Sorting Data...\n","output_type":"stream"},{"name":"stderr","text":"Sorting Series: 100%|██████████| 4348/4348 [52:21<00:00,  1.38it/s]  \n","output_type":"stream"},{"name":"stdout","text":"Saved series_index_mapping.csv\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ==========================================\n# 4. PREPARE CONVERSION LIST (Positive Only logic)\n# ==========================================\nprint(\">>> Step 2: filtering Positive Cases...\")\n\nexclude_cols = ['SeriesInstanceUID', 'PatientAge', 'PatientSex', 'Modality', 'Aneurysm Present']\nlocation_cols = [col for col in df_train.columns if col not in exclude_cols]\n\noutputList = []\nunique_series = df_train['SeriesInstanceUID'].unique()\n\nfor si in tqdm(unique_series, desc=\"Filtering Positives\"):\n    pdf = df_train[df_train['SeriesInstanceUID'] == si]\n    \n    # Check if this series has any aneurysm (Using your original logic)\n    has_location = False\n    for _, row in pdf.iterrows():\n        # Find locations where value is 1\n        active_locs = [col for col in location_cols if row[col] == 1]\n        \n        if active_locs:\n            has_location = True\n            # Get sorted DICOMs for this series\n            df_series_map = df_series_index_mapping[df_series_index_mapping['SeriesInstanceUID'] == si]\n            \n            for loc in active_locs:\n                clean_loc = loc.replace('/', '_')\n                out_folder = os.path.join(OUTPUT_DIR, clean_loc, si)\n                \n                # Add all frames for this series to the processing list\n                for row_map in df_series_map.itertuples():\n                    dst = os.path.join(out_folder, f\"{row_map.relative_index:04d}.png\")\n                    outputList.append({\n                        'impath': row_map.dicom_filename,\n                        'dst': dst,\n                        'modality': row_map.Modality\n                    })\n            break # Optimization: if found, move to processing\n\nprint(f\"Total images to convert: {len(outputList)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T20:45:39.481451Z","iopub.execute_input":"2025-11-13T20:45:39.482186Z","iopub.status.idle":"2025-11-13T20:48:17.401813Z","shell.execute_reply.started":"2025-11-13T20:45:39.482143Z","shell.execute_reply":"2025-11-13T20:48:17.400739Z"}},"outputs":[{"name":"stdout","text":">>> Step 2: filtering Positive Cases...\n","output_type":"stream"},{"name":"stderr","text":"Filtering Positives: 100%|██████████| 4348/4348 [02:37<00:00, 27.54it/s]","output_type":"stream"},{"name":"stdout","text":"Total images to convert: 577572\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ==========================================\n# 5. DICOM TO PNG CONVERSION\n# ==========================================\ndef dicom_to_png(src_path, dst_path, width, modality):\n    try:\n        if os.path.exists(dst_path): return \n        \n        ds = pydicom.dcmread(src_path, force=True)\n        img = ds.pixel_array.astype(np.float32)\n        \n        # Handle Rescale\n        if hasattr(ds, 'RescaleSlope') and hasattr(ds, 'RescaleIntercept'):\n            img = img * float(ds.RescaleSlope) + float(ds.RescaleIntercept)\n            \n        # Apply Windowing\n        wc, ww = get_windowing_params(modality)\n        img = apply_dicom_windowing(img, wc, ww)\n        \n        # Resize\n        img = cv2.resize(img, (width, width), interpolation=cv2.INTER_AREA)\n        \n        # Save\n        os.makedirs(os.path.dirname(dst_path), exist_ok=True)\n        cv2.imwrite(dst_path, img)\n        \n    except Exception as e:\n        # Silent fail to keep logs clean, or print e if debugging\n        pass\n\nprint(\">>> Step 3: Converting DICOM to PNG (Parallel)...\")\nstart_time = time.time()\n\nParallel(n_jobs=cpu_count())(\n    delayed(dicom_to_png)(item['impath'], item['dst'], GLOBAL_WIDTH, item['modality'])\n    for item in tqdm(outputList)\n)\n\nprint(f\"Conversion finished in {(time.time() - start_time)/60:.2f} minutes.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T20:48:22.706445Z","iopub.execute_input":"2025-11-13T20:48:22.706773Z","iopub.status.idle":"2025-11-13T22:23:55.082462Z","shell.execute_reply.started":"2025-11-13T20:48:22.706746Z","shell.execute_reply":"2025-11-13T22:23:55.079129Z"}},"outputs":[{"name":"stdout","text":">>> Step 3: Converting DICOM to PNG (Parallel)...\n","output_type":"stream"},{"name":"stderr","text":" 85%|████████▍ | 490356/577572 [1:20:43<18:26, 78.80it/s]  /usr/local/lib/python3.11/dist-packages/pydicom/pixels/utils.py:222: UserWarning: A value of 'None' for (0028,0008) 'Number of Frames' is invalid, assuming 1 frame\n  warn_and_log(\n/usr/local/lib/python3.11/dist-packages/pydicom/pixels/utils.py:222: UserWarning: A value of 'None' for (0028,0008) 'Number of Frames' is invalid, assuming 1 frame\n  warn_and_log(\n/usr/local/lib/python3.11/dist-packages/pydicom/pixels/utils.py:222: UserWarning: A value of 'None' for (0028,0008) 'Number of Frames' is invalid, assuming 1 frame\n  warn_and_log(\n/usr/local/lib/python3.11/dist-packages/pydicom/pixels/utils.py:222: UserWarning: A value of 'None' for (0028,0008) 'Number of Frames' is invalid, assuming 1 frame\n  warn_and_log(\n100%|██████████| 577572/577572 [1:35:31<00:00, 100.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Conversion finished in 95.54 minutes.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================\n# 6. CREATE LOCALIZERS CSV (Relative Coords)\n# ==========================================\nprint(\">>> Step 4: Processing Localizers...\")\ndf_localizers = pd.read_csv(f'{RD}/train_localizers.csv')\n\n# Create a quick lookup dictionary\nmapping_dict = {\n    (row.SeriesInstanceUID, row.SOPInstanceUID): (row.relative_index, row.dicom_filename)\n    for row in df_series_index_mapping.itertuples()\n}\n\nrel_indices, rel_xs, rel_ys = [], [], []\n\nfor _, row in tqdm(df_localizers.iterrows(), total=len(df_localizers)):\n    key = (row['SeriesInstanceUID'], row['SOPInstanceUID'])\n    \n    if key in mapping_dict:\n        idx, path = mapping_dict[key]\n        rel_indices.append(idx)\n        \n        try:\n            # We need image dimensions to calculate relative coordinates\n            # Optimization: Only read header\n            ds = pydicom.dcmread(path, stop_before_pixels=True)\n            h, w = int(ds.Rows), int(ds.Columns)\n            \n            coords = eval(row['coordinates']) if isinstance(row['coordinates'], str) else row['coordinates']\n            rel_xs.append((coords['x'] / w) * GLOBAL_WIDTH)\n            rel_ys.append((coords['y'] / h) * GLOBAL_WIDTH)\n        except:\n            rel_xs.append(None)\n            rel_ys.append(None)\n    else:\n        rel_indices.append(None)\n        rel_xs.append(None)\n        rel_ys.append(None)\n\ndf_localizers['relative_index'] = rel_indices\ndf_localizers['relative_x'] = rel_xs\ndf_localizers['relative_y'] = rel_ys\ndf_localizers.to_csv('train_localizers_with_relative.csv', index=False)\nprint(\"Saved train_localizers_with_relative.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:23:59.897463Z","iopub.execute_input":"2025-11-13T22:23:59.899572Z","iopub.status.idle":"2025-11-13T22:24:27.540706Z","shell.execute_reply.started":"2025-11-13T22:23:59.899531Z","shell.execute_reply":"2025-11-13T22:24:27.539478Z"}},"outputs":[{"name":"stdout","text":">>> Step 4: Processing Localizers...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2254/2254 [00:23<00:00, 96.96it/s] \n","output_type":"stream"},{"name":"stdout","text":"Saved train_localizers_with_relative.csv\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================\n# 7. FINAL STEP: CREATE DOWNLOADABLE DATASET\n# ==========================================\nprint(\">>> Step 5: Zipping Output for Download...\")\n\n# We Zip the PNG folder and the created CSVs into one file\n# -r: recursive, -q: quiet\nos.system(\"zip -r -q dataset.zip cvt_png series_index_mapping.csv train_localizers_with_relative.csv\")\n\nprint(\"Done! You can now download 'dataset.zip' from the Output tab.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T22:24:33.750363Z","iopub.execute_input":"2025-11-13T22:24:33.750699Z","iopub.status.idle":"2025-11-13T22:36:34.180724Z","shell.execute_reply.started":"2025-11-13T22:24:33.750675Z","shell.execute_reply":"2025-11-13T22:36:34.177327Z"}},"outputs":[{"name":"stdout","text":">>> Step 5: Zipping Output for Download...\nDone! You can now download 'dataset.zip' from the Output tab.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}