{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cb1f218",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T08:44:40.588175Z",
     "iopub.status.busy": "2025-11-15T08:44:40.587598Z",
     "iopub.status.idle": "2025-11-15T08:44:56.888964Z",
     "shell.execute_reply": "2025-11-15T08:44:56.888116Z"
    },
    "papermill": {
     "duration": 16.306503,
     "end_time": "2025-11-15T08:44:56.890329",
     "exception": false,
     "start_time": "2025-11-15T08:44:40.583826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful. Numpy version: 1.26.4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import functools\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  # <-- This will now work\n",
    "from typing import List, Tuple, Optional\n",
    "import gc\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import timm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold  # <-- This will now work\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pydicom\n",
    "import polars as pl\n",
    "\n",
    "# The RSNA inference server (needed for submission)\n",
    "try:\n",
    "    import kaggle_evaluation.rsna_inference_server as rsna_inference_server\n",
    "except ImportError:\n",
    "    print(\"Inference server not found (OK for training)\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"Imports successful. Numpy version: {np.__version__}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a8e0074",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T08:44:56.896612Z",
     "iopub.status.busy": "2025-11-15T08:44:56.896407Z",
     "iopub.status.idle": "2025-11-15T08:44:56.902466Z",
     "shell.execute_reply": "2025-11-15T08:44:56.901909Z"
    },
    "papermill": {
     "duration": 0.010318,
     "end_time": "2025-11-15T08:44:56.903424",
     "exception": false,
     "start_time": "2025-11-15T08:44:56.893106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------\n",
    "# 2. GLOBAL CONFIGURATION\n",
    "# -------------------------\n",
    "class Config:\n",
    "    # --- IMPORTANT: Path to your PNG dataset ---\n",
    "    # This path comes from your screenshot\n",
    "    DATA_DIR = \"/kaggle/input/another1\" \n",
    "    \n",
    "    # --- Paths derived from DATA_DIR ---\n",
    "    CVT_PNG_DIR = os.path.join(DATA_DIR, \"cvt_png\")\n",
    "    SERIES_MAPPING_PATH = os.path.join(DATA_DIR, \"series_index_mapping.csv\")\n",
    "    LOCALIZERS_PATH = os.path.join(DATA_DIR, \"train_localizers_with_relative.csv\")\n",
    "    \n",
    "    # --- Path to original competition data ---\n",
    "    ORIGINAL_DATA_DIR = \"/kaggle/input/rsna-intracranial-aneurysm-detection\"\n",
    "    TRAIN_CSV_PATH = os.path.join(ORIGINAL_DATA_DIR, \"train.csv\")\n",
    "    ORIGINAL_SERIES_DIR = os.path.join(ORIGINAL_DATA_DIR, \"series\")\n",
    "    \n",
    "    # --- Model Hyperparameters ---\n",
    "    NUM_FRAMES = 8\n",
    "    IMAGE_SIZE = 224\n",
    "    NUM_CLASSES = 14\n",
    "    BATCH_SIZE = 6\n",
    "    NUM_EPOCHS = 10\n",
    "    LEARNING_RATE = 5e-5\n",
    "    MODEL_NAME_BACKBONE = \"tf_efficientnetv2_s.in1k\"\n",
    "    \n",
    "    # --- Feature Flags ---\n",
    "    USE_METADATA = True\n",
    "    USE_WINDOWING = True\n",
    "    USE_3CHANNEL_INPUT = True\n",
    "    USE_IMPROVED_LOSS = True\n",
    "    USE_CLAHE = True\n",
    "    USE_STRONG_AUGMENTATION = True\n",
    "    \n",
    "    # --- Dataloader & System ---\n",
    "    NUM_WORKERS = 2\n",
    "    PIN_MEMORY = True\n",
    "    PREFETCH_FACTOR = 2\n",
    "    PERSISTENT_WORKERS = True\n",
    "    \n",
    "    # --- CV & Training Loop ---\n",
    "    NUM_FOLDS = 5 # Set to 1 if you want to train only one model\n",
    "    FOLD = 0 # We will only train FOLD 0 if TRAIN_MODEL is True\n",
    "    ACCUMULATION_STEPS = 5\n",
    "    EARLY_STOPPING_PATIENCE = 5\n",
    "    USE_GROUP_CV = True\n",
    "    CACHE_SIZE = 100\n",
    "    OUTPUT_DIR = \"/kaggle/working\"\n",
    "    MODEL_NAME = \"eightframe_efficientnetv2s\"\n",
    "\n",
    "config = Config()\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 4. GLOBAL TARGETS\n",
    "# -------------------------\n",
    "TARGET_COLS = [\n",
    "    'Left Infraclinoid Internal Carotid Artery',\n",
    "    'Right Infraclinoid Internal Carotid Artery', \n",
    "    'Left Supraclinoid Internal Carotid Artery',\n",
    "    'Right Supraclinoid Internal Carotid Artery',\n",
    "    'Left Middle Cerebral Artery',\n",
    "    'Right Middle Cerebral Artery',\n",
    "    'Anterior Communicating Artery',\n",
    "    'Left Anterior Cerebral Artery',\n",
    "    'Right Anterior Cerebral Artery', \n",
    "    'Left Posterior Communicating Artery',\n",
    "    'Right Posterior Communicating Artery',\n",
    "    'Basilar Tip',\n",
    "    'Other Posterior Circulation',\n",
    "    'Aneurysm Present'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06c154fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T08:44:56.908978Z",
     "iopub.status.busy": "2025-11-15T08:44:56.908593Z",
     "iopub.status.idle": "2025-11-15T08:44:57.027916Z",
     "shell.execute_reply": "2025-11-15T08:44:57.027161Z"
    },
    "papermill": {
     "duration": 0.12331,
     "end_time": "2025-11-15T08:44:57.029040",
     "exception": false,
     "start_time": "2025-11-15T08:44:56.905730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------\n",
    "# 3. GLOBAL SEED & DEVICE\n",
    "# -------------------------\n",
    "def set_seed(seed: int = 42, deterministic: bool = False):\n",
    "    \"\"\"Set all random seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    if deterministic:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    else:\n",
    "        torch.backends.cudnn.deterministic = False\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "set_seed(42, deterministic=False)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    torch.cuda.empty_cache()\n",
    "elif TRAIN_MODEL:\n",
    "    print(\"WARNING: CUDA not available. Training will be VERY slow.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90035322",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T08:44:57.035335Z",
     "iopub.status.busy": "2025-11-15T08:44:57.035095Z",
     "iopub.status.idle": "2025-11-15T08:44:57.057315Z",
     "shell.execute_reply": "2025-11-15T08:44:57.056571Z"
    },
    "papermill": {
     "duration": 0.026784,
     "end_time": "2025-11-15T08:44:57.058370",
     "exception": false,
     "start_time": "2025-11-15T08:44:57.031586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\t#-------------------------\n",
    "# 5. GLOBAL HELPER FUNCTIONS\n",
    "# -------------------------\n",
    "def get_windowing_params(modality: str) -> Tuple[float, float]:\n",
    "    windows = {\n",
    "        'CT': (40, 80), 'CTA': (50, 350), 'MRA': (600, 1200),\n",
    "        'MRI': (40, 80), 'MR': (40, 80)\n",
    "    }\n",
    "    return windows.get(modality, (40, 80))\n",
    "\n",
    "def apply_dicom_windowing(img: np.ndarray, window_center: float, window_width: float) -> np.ndarray:\n",
    "    img_min = window_center - window_width // 2\n",
    "    img_max = window_center + window_width // 2\n",
    "    img = np.clip(img, img_min, img_max)\n",
    "    img = (img - img_min) / (img_max - img_min + 1e-7)\n",
    "    return (img * 255).astype(np.uint8)\n",
    "\n",
    "def apply_clahe_normalization(img: np.ndarray, modality: str) -> np.ndarray:\n",
    "    if not config.USE_CLAHE:\n",
    "        return img.astype(np.uint8)\n",
    "        \n",
    "    img = img.astype(np.uint8)\n",
    "    if modality in ['CTA', 'MRA']:\n",
    "        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "        img_clahe = clahe.apply(img)\n",
    "        img_clahe = cv2.convertScaleAbs(img_clahe, alpha=1.1, beta=5)\n",
    "    elif modality in ['MRI', 'MR']:\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        img_clahe = clahe.apply(img)\n",
    "        img_clahe = np.power(img_clahe / 255.0, 0.9) * 255\n",
    "        img_clahe = img_clahe.astype(np.uint8)\n",
    "    else:\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.5, tileGridSize=(8, 8))\n",
    "        img_clahe = clahe.apply(img)\n",
    "    return img_clahe\n",
    "\n",
    "def robust_normalization(volume: np.ndarray) -> np.ndarray:\n",
    "    p1, p99 = np.percentile(volume.flatten(), [1, 99])\n",
    "    volume_norm = np.clip(volume, p1, p99)\n",
    "    if p99 > p1:\n",
    "        volume_norm = (volume_norm - p1) / (p99 - p1 + 1e-7)\n",
    "    else:\n",
    "        volume_norm = np.zeros_like(volume_norm)\n",
    "    return (volume_norm * 255).astype(np.uint8)\n",
    "\n",
    "def create_3channel_input_8frame(volume: np.ndarray) -> np.ndarray:\n",
    "    if len(volume) == 0:\n",
    "        return np.zeros((config.IMAGE_SIZE, config.IMAGE_SIZE, 3), dtype=np.uint8)\n",
    "    \n",
    "    middle_slice = volume[len(volume) // 2]\n",
    "    mip = np.max(volume, axis=0)\n",
    "    std_proj = np.std(volume, axis=0).astype(np.float32)\n",
    "    \n",
    "    if std_proj.max() > std_proj.min():\n",
    "        p1, p99 = np.percentile(std_proj, [5, 95])\n",
    "        std_proj = np.clip(std_proj, p1, p99)\n",
    "        std_proj = ((std_proj - p1) / (p99 - p1 + 1e-7) * 255).astype(np.uint8)\n",
    "    else:\n",
    "        std_proj = np.zeros_like(std_proj, dtype=np.uint8)\n",
    "        \n",
    "    return np.stack([middle_slice, mip, std_proj], axis=-1)\n",
    "\n",
    "def smart_8_frame_sampling(volume_paths: List[str]) -> List[str]:\n",
    "    n = len(volume_paths)\n",
    "    if n == 0:\n",
    "        return []\n",
    "    if n <= 8:\n",
    "        result = volume_paths[:]\n",
    "        while len(result) < 8:\n",
    "            result.extend(volume_paths[:8-len(result)])\n",
    "        return result[:8]\n",
    "\n",
    "    start_idx = max(0, int(n * 0.1))\n",
    "    available_frames = n - start_idx\n",
    "    step = max(1, available_frames // 8)\n",
    "    indices = []\n",
    "    current_idx = start_idx\n",
    "    \n",
    "    while len(indices) < 8 and current_idx < n:\n",
    "        indices.append(current_idx)\n",
    "        current_idx += step\n",
    "        \n",
    "    while len(indices) < 8:\n",
    "        remaining = [i for i in range(n) if i not in indices]\n",
    "        if remaining:\n",
    "            indices.append(remaining[len(indices) % len(remaining)])\n",
    "        else:\n",
    "            indices.append(indices[-1])\n",
    "            \n",
    "    return [volume_paths[i] for i in indices[:8]]\n",
    "\n",
    "def resolve_dicom_path(dicom_entry, series_uid):\n",
    "    \"\"\" Robustly find DICOM path. \"\"\"\n",
    "    if dicom_entry and os.path.exists(dicom_entry):\n",
    "        return dicom_entry\n",
    "        \n",
    "    # Try relative to original series dir\n",
    "    series_dir = os.path.join(config.ORIGINAL_SERIES_DIR, series_uid)\n",
    "    if dicom_entry:\n",
    "        possible = os.path.join(series_dir, os.path.basename(dicom_entry))\n",
    "        if os.path.exists(possible):\n",
    "            return possible\n",
    "\n",
    "    # Try searching series dir\n",
    "    if os.path.exists(series_dir):\n",
    "        candidates = sorted(glob.glob(os.path.join(series_dir, \"*.dcm\")))\n",
    "        if candidates:\n",
    "            # Fallback: try to find by SOPInstanceUID if entry is just a filename\n",
    "            if dicom_entry:\n",
    "                sop_uid = os.path.splitext(os.path.basename(dicom_entry))[0]\n",
    "                for cand_path in candidates:\n",
    "                    if sop_uid in cand_path:\n",
    "                        return cand_path\n",
    "            return candidates[0] # Return first if no match\n",
    "\n",
    "    raise FileNotFoundError(f\"No DICOM file found for series {series_uid}. Tried entry='{dicom_entry}'\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 6. GLOBAL MODEL & LOSS DEFINITION\n",
    "# -------------------------\n",
    "class ImprovedMultiFrameModel(nn.Module):\n",
    "    def __init__(self, num_frames=8, num_classes=14, pretrained=True):\n",
    "        super(ImprovedMultiFrameModel, self).__init__()\n",
    "        self.num_frames = num_frames\n",
    "        self.num_classes = num_classes\n",
    "        self.use_metadata = config.USE_METADATA\n",
    "        print(f\"Loading backbone: {config.MODEL_NAME_BACKBONE}\")\n",
    "        self.backbone = timm.create_model(\n",
    "            config.MODEL_NAME_BACKBONE,\n",
    "            pretrained=pretrained,\n",
    "            num_classes=0,\n",
    "            global_pool='avg'\n",
    "        )\n",
    "        self.feature_dim = self.backbone.num_features\n",
    "        print(f\"Backbone {config.MODEL_NAME_BACKBONE}: {self.feature_dim} features\")\n",
    "        \n",
    "        if self.use_metadata:\n",
    "            self.meta_fc = nn.Sequential(\n",
    "                nn.Linear(2, 16),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(16, 32),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            classifier_input_dim = self.feature_dim + 32\n",
    "        else:\n",
    "            classifier_input_dim = self.feature_dim\n",
    "            \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(classifier_input_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, meta=None):\n",
    "        features = self.backbone(x)\n",
    "        if self.use_metadata and meta is not None:\n",
    "            meta_features = self.meta_fc(meta)\n",
    "            features = torch.cat([features, meta_features], dim=1)\n",
    "        output = self.classifier(features)\n",
    "        return output\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * bce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "class ImprovedLoss(nn.Module):\n",
    "    def __init__(self, aneurysm_weight=3.0, focal_weight=0.3):\n",
    "        super(ImprovedLoss, self).__init__()\n",
    "        self.aneurysm_weight = aneurysm_weight\n",
    "        self.focal_weight = focal_weight\n",
    "        # Create weights on the correct device\n",
    "        weights = torch.ones(config.NUM_CLASSES)\n",
    "        weights[-1] = aneurysm_weight\n",
    "        self.register_buffer('weights', weights)\n",
    "        self.focal_loss = FocalLoss(alpha=1, gamma=2)\n",
    "        \n",
    "    def forward(self, outputs, targets):\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(outputs, targets, reduction='none')\n",
    "        weighted_bce = (bce_loss * self.weights).mean()\n",
    "        focal_loss_val = self.focal_loss(outputs, targets)\n",
    "        return (1 - self.focal_weight) * weighted_bce + self.focal_weight * focal_loss_val\n",
    "\n",
    "\t\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af907e56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T08:44:57.064484Z",
     "iopub.status.busy": "2025-11-15T08:44:57.064014Z",
     "iopub.status.idle": "2025-11-15T08:44:57.108488Z",
     "shell.execute_reply": "2025-11-15T08:44:57.107664Z"
    },
    "papermill": {
     "duration": 0.04889,
     "end_time": "2025-11-15T08:44:57.109594",
     "exception": false,
     "start_time": "2025-11-15T08:44:57.060704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "STARTING TRAINING MODE (Internet ON)\n",
      "========================================\n",
      "Using strong augmentation (minus Elastic/GridDistortion)...\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# ------------------------- PHASE 1: TRAINING -------------------------\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "print(\"=\"*40)\n",
    "print(\"STARTING TRAINING MODE (Internet ON)\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# -------------------------\n",
    "# 7. TRAINING: DATASET CLASS\n",
    "# -------------------------\n",
    "def create_frame_paths_8frame_structured(train_df, series_mapping_df):\n",
    "    \"\"\"Return dict: series_uid -> {'paths': [...], 'is_dummy': bool}\"\"\"\n",
    "    frame_paths = {}\n",
    "    print(\"Creating 8-frame optimized structured paths from PNG dataset...\")\n",
    "    \n",
    "    # Pre-build a lookup for all PNGs\n",
    "    all_png_files = {}\n",
    "    print(f\"Scanning PNG directory: {config.CVT_PNG_DIR}\")\n",
    "    if not os.path.exists(config.CVT_PNG_DIR):\n",
    "         print(f\"WARNING: PNG Directory not found at {config.CVT_PNG_DIR}\")\n",
    "         print(\"The dataset will fall back to reading DICOMs, which will be much slower.\")\n",
    "    \n",
    "    for root, _, files in os.walk(config.CVT_PNG_DIR):\n",
    "        for file in files:\n",
    "            if file.endswith(\".png\"):\n",
    "                # Path: .../cvt_png/DISEASE_NAME/SERIES_UID/FRAME.png\n",
    "                try:\n",
    "                    series_uid = os.path.basename(os.path.dirname(root))\n",
    "                    if series_uid not in all_png_files:\n",
    "                        all_png_files[series_uid] = []\n",
    "                    all_png_files[series_uid].append(os.path.join(root, file))\n",
    "                except:\n",
    "                    pass\n",
    "    print(f\"Found PNGs for {len(all_png_files)} series.\")\n",
    "\n",
    "    for series_uid in tqdm(train_df['SeriesInstanceUID'].unique(), desc=\"Processing series\"):\n",
    "        series_data = series_mapping_df[series_mapping_df['SeriesInstanceUID'] == series_uid]\n",
    "        if series_data.empty:\n",
    "            frame_paths[series_uid] = {'paths': [], 'is_dummy': True}\n",
    "            continue\n",
    "\n",
    "        # Try to find pre-converted PNGs\n",
    "        found_paths = []\n",
    "        if series_uid in all_png_files:\n",
    "            # We found PNGs. Sort them numerically by filename (e.g., 0001.png, 0002.png)\n",
    "            # Use set() to remove duplicates if a series is in multiple disease folders\n",
    "            png_files = sorted(list(set(all_png_files[series_uid])), key=lambda x: os.path.basename(x))\n",
    "            if png_files:\n",
    "                found_paths = png_files\n",
    "        \n",
    "        # Fallback: Use DICOMs (marked as 'dummy' PNG path)\n",
    "        if not found_paths:\n",
    "            dicom_dir = os.path.join(config.ORIGINAL_SERIES_DIR, series_uid)\n",
    "            if os.path.exists(dicom_dir):\n",
    "                num_frames = len(series_data)\n",
    "                # Create dummy paths that signal DICOM loading\n",
    "                found_paths = [f\"dummy_dicom_path_{i:04d}.dcm\" for i in range(num_frames)]\n",
    "\n",
    "        if found_paths:\n",
    "            sampled = smart_8_frame_sampling(found_paths)\n",
    "            is_dummy = any(p.startswith('dummy_dicom_path') for p in sampled)\n",
    "            frame_paths[series_uid] = {'paths': sampled, 'is_dummy': is_dummy}\n",
    "        else:\n",
    "            frame_paths[series_uid] = {'paths': [], 'is_dummy': True}\n",
    "            \n",
    "    return frame_paths\n",
    "\n",
    "\n",
    "class EightFrameDataset(Dataset):\n",
    "    def __init__(self, df, frame_paths_dict, series_mapping_df, num_frames=8, transform=None, is_training=True):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.frame_paths_dict = frame_paths_dict\n",
    "        self.series_mapping_df = series_mapping_df\n",
    "        self.num_frames = num_frames\n",
    "        self.transform = transform\n",
    "        self.is_training = is_training\n",
    "        self._cache = {}\n",
    "        self._cache_keys = []\n",
    "        self._max_cache_size = config.CACHE_SIZE\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx in self._cache:\n",
    "            return self._cache[idx]\n",
    "        \n",
    "        row = self.df.iloc[idx]\n",
    "        series_uid = row['SeriesInstanceUID']\n",
    "        labels = torch.tensor(row[TARGET_COLS].values.astype(np.float32))\n",
    "        metadata = self._extract_metadata(row)\n",
    "        \n",
    "        try:\n",
    "            image = self._load_8frame_3channel_image(series_uid, row)\n",
    "        except Exception as e:\n",
    "            # print(f\"Error loading {series_uid}: {e}. Returning zeros.\") # Uncomment for debug\n",
    "            dummy_image = np.zeros((config.IMAGE_SIZE, config.IMAGE_SIZE, 3), dtype=np.uint8)\n",
    "            if self.transform:\n",
    "                image = self.transform(image=dummy_image)['image']\n",
    "            else:\n",
    "                image = torch.from_numpy(dummy_image).permute(2,0,1).float()\n",
    "\n",
    "        result = (image, labels, metadata)\n",
    "        self._update_cache(idx, result)\n",
    "        return result\n",
    "\n",
    "    def _update_cache(self, idx, data):\n",
    "        if len(self._cache) >= self._max_cache_size:\n",
    "            oldest_idx = self._cache_keys.pop(0)\n",
    "            if oldest_idx in self._cache:\n",
    "                del self._cache[oldest_idx]\n",
    "        self._cache[idx] = data\n",
    "        self._cache_keys.append(idx)\n",
    "\n",
    "    def _extract_metadata(self, row) -> torch.Tensor:\n",
    "        if not config.USE_METADATA:\n",
    "            return torch.tensor([0.0, 0.0], dtype=torch.float32)\n",
    "        \n",
    "        age = row.get('PatientAge', 50)\n",
    "        if pd.isna(age):\n",
    "            age = 50\n",
    "        elif isinstance(age, str):\n",
    "            age = int(''.join(filter(str.isdigit, age[:3])) or '50')\n",
    "        age = min(float(age), 100.0) / 100.0\n",
    "        \n",
    "        sex = row.get('PatientSex', 'M')\n",
    "        sex = 1.0 if sex == 'M' else 0.0\n",
    "        return torch.tensor([age, sex], dtype=torch.float32)\n",
    "\n",
    "    def _load_8frame_3channel_image(self, series_uid: str, row) -> torch.Tensor:\n",
    "        entry = self.frame_paths_dict.get(series_uid, {'paths': [], 'is_dummy': True})\n",
    "        paths = entry['paths']\n",
    "        is_dummy = entry.get('is_dummy', True)\n",
    "        \n",
    "        if len(paths) == 0:\n",
    "            raise FileNotFoundError(f\"No paths found for series {series_uid}\")\n",
    "            \n",
    "        if is_dummy:\n",
    "            # is_dummy=True means we must load from DICOM\n",
    "            volume = self._load_volume_from_dicom_8frame(series_uid, row)\n",
    "        else:\n",
    "            # We have PNGs, load them\n",
    "            volume = self._load_volume_from_png_8frame(paths)\n",
    "            \n",
    "        volume_norm = robust_normalization(volume)\n",
    "        image = create_3channel_input_8frame(volume_norm)\n",
    "        \n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image)\n",
    "            image = transformed['image']\n",
    "            \n",
    "        return image\n",
    "\n",
    "    def _load_volume_from_png_8frame(self, paths: List[str]) -> np.ndarray:\n",
    "        volume = []\n",
    "        for path in paths:\n",
    "            try:\n",
    "                img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "                if img is not None:\n",
    "                    img = cv2.resize(img, (config.IMAGE_SIZE, config.IMAGE_SIZE), interpolation=cv2.INTER_AREA)\n",
    "                    volume.append(img)\n",
    "                else:\n",
    "                    volume.append(np.zeros((config.IMAGE_SIZE, config.IMAGE_SIZE), dtype=np.uint8))\n",
    "            except Exception:\n",
    "                volume.append(np.zeros((config.IMAGE_SIZE, config.IMAGE_SIZE), dtype=np.uint8))\n",
    "        \n",
    "        if not volume:\n",
    "            return np.zeros((8, config.IMAGE_SIZE, config.IMAGE_SIZE), dtype=np.uint8)\n",
    "        return np.array(volume)\n",
    "\n",
    "    def _load_volume_from_dicom_8frame(self, series_uid: str, row) -> np.ndarray:\n",
    "        series_data = self.series_mapping_df[self.series_mapping_df['SeriesInstanceUID'] == series_uid].sort_values('relative_index')\n",
    "        if series_data.empty:\n",
    "            return np.zeros((8, config.IMAGE_SIZE, config.IMAGE_SIZE), dtype=np.uint8)\n",
    "            \n",
    "        modality = row.get('Modality', 'CT')\n",
    "        \n",
    "        # Smart sampling logic on the series_data dataframe\n",
    "        if len(series_data) <= 8:\n",
    "            sampled_data = series_data\n",
    "        else:\n",
    "            all_indices = list(range(len(series_data)))\n",
    "            sampled_indices_str = smart_8_frame_sampling([str(i) for i in all_indices])\n",
    "            sampled_indices = [int(i) for i in sampled_indices_str]\n",
    "            sampled_data = series_data.iloc[sampled_indices]\n",
    "            \n",
    "        volume = []\n",
    "        for _, dicom_row in sampled_data.iterrows():\n",
    "            dicom_entry = dicom_row.get('dicom_filename', None)\n",
    "            try:\n",
    "                dicom_path = resolve_dicom_path(dicom_entry, series_uid)\n",
    "                ds = pydicom.dcmread(dicom_path, force=True)\n",
    "                img = ds.pixel_array.astype(np.float32)\n",
    "                \n",
    "                if hasattr(ds, 'RescaleSlope') and hasattr(ds, 'RescaleIntercept'):\n",
    "                    img = img * float(ds.RescaleSlope) + float(ds.RescaleIntercept)\n",
    "                    \n",
    "                if config.USE_WINDOWING:\n",
    "                    window_center, window_width = get_windowing_params(modality)\n",
    "                    img = apply_dicom_windowing(img, window_center, window_width)\n",
    "                else: # Simple normalization\n",
    "                    img_min, img_max = img.min(), img.max()\n",
    "                    if img_max > img_min:\n",
    "                        img = ((img - img_min) / (img_max - img_min) * 255).astype(np.uint8)\n",
    "                    else:\n",
    "                        img = np.zeros_like(img, dtype=np.uint8)\n",
    "                        \n",
    "                img = apply_clahe_normalization(img, modality)\n",
    "                img = cv2.resize(img, (config.IMAGE_SIZE, config.IMAGE_SIZE), interpolation=cv2.INTER_AREA)\n",
    "                volume.append(img)\n",
    "            except Exception as e:\n",
    "                # print(f\"Dicom read error for {series_uid}: {e}\") # Uncomment for debug\n",
    "                volume.append(np.zeros((config.IMAGE_SIZE, config.IMAGE_SIZE), dtype=np.uint8))\n",
    "        \n",
    "        # Ensure 8 frames\n",
    "        while len(volume) < 8:\n",
    "            if volume:\n",
    "                volume.append(volume[-1])\n",
    "            else:\n",
    "                volume.append(np.zeros((config.IMAGE_SIZE, config.IMAGE_SIZE), dtype=np.uint8))\n",
    "                \n",
    "        return np.array(volume[:8])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 8. TRAINING: TRANSFORMS\n",
    "# -------------------------\n",
    "\n",
    "# FIX: Removed ElasticTransform and GridDistortion as they \n",
    "# are the specific augmentations that trigger the scipy import error.\n",
    "\n",
    "if config.USE_STRONG_AUGMENTATION:\n",
    "    print(\"Using strong augmentation (minus Elastic/GridDistortion)...\")\n",
    "    train_transform = A.Compose([\n",
    "        A.Rotate(limit=15, p=0.7),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=10, p=0.6),\n",
    "        \n",
    "        A.ElasticTransform(alpha=50, sigma=5, p=0.3),     # <-- REMOVED: This causes the scipy error\n",
    "        A.GridDistortion(num_steps=3, distort_limit=0.1, p=0.3), # <-- REMOVED: This also causes the error\n",
    "        \n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.6),\n",
    "        A.RandomGamma(gamma_limit=(80, 120), p=0.4),\n",
    "        A.GaussNoise(var_limit=(10, 80), p=0.4),\n",
    "        A.Blur(blur_limit=3, p=0.2),\n",
    "        A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.3),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "else:\n",
    "    print(\"Using standard augmentation...\")\n",
    "    train_transform = A.Compose([\n",
    "        A.Rotate(limit=10, p=0.5),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.3),\n",
    "        A.GaussNoise(var_limit=(10, 50), p=0.2),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 9. TRAINING: LOSS & METRIC\n",
    "# -------------------------\n",
    "def get_loss_function():\n",
    "    if config.USE_IMPROVED_LOSS:\n",
    "        return ImprovedLoss(aneurysm_weight=3.0, focal_weight=0.3).to(device)\n",
    "    else:\n",
    "        weights = torch.ones(config.NUM_CLASSES, device=device)\n",
    "        weights[-1] = 3.0\n",
    "        return nn.BCEWithLogitsLoss(pos_weight=weights)\n",
    "\n",
    "def calculate_competition_metric(y_true, y_pred):\n",
    "    individual_aucs = []\n",
    "    for i in range(13):\n",
    "        try:\n",
    "            if len(np.unique(y_true[:, i])) > 1:\n",
    "                auc = roc_auc_score(y_true[:, i], y_pred[:, i])\n",
    "            else:\n",
    "                auc = 0.5\n",
    "            individual_aucs.append(auc)\n",
    "        except:\n",
    "            individual_aucs.append(0.5)\n",
    "            \n",
    "    try:\n",
    "        if len(np.unique(y_true[:, 13])) > 1:\n",
    "            aneurysm_present_auc = roc_auc_score(y_true[:, 13], y_pred[:, 13])\n",
    "        else:\n",
    "            aneurysm_present_auc = 0.5\n",
    "    except:\n",
    "        aneurysm_present_auc = 0.5\n",
    "        \n",
    "    avg_individual = np.mean(individual_aucs)\n",
    "    final_score = (aneurysm_present_auc + avg_individual) / 2\n",
    "    return final_score, aneurysm_present_auc, avg_individual\n",
    "\n",
    "# -------------------------\n",
    "# 10. TRAINING: HELPERS\n",
    "# -------------------------\n",
    "def save_checkpoint(model, optimizer, scheduler, epoch, best_score, val_loss, out_dir, model_name, fold):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    model_path = os.path.join(\n",
    "        out_dir,\n",
    "        f\"{model_name}_fold{fold}_epoch{epoch}_score{best_score:.6f}.pth\"\n",
    "    )\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'best_score': best_score,\n",
    "        'val_loss': val_loss,\n",
    "    }, model_path)\n",
    "    return model_path\n",
    "\n",
    "@functools.lru_cache(maxsize=5000)\n",
    "def extract_dicom_patient_info(series_uid: str) -> Tuple[str, str]:\n",
    "    \"\"\"Reads PatientID from the original DICOM files.\"\"\"\n",
    "    try:\n",
    "        dicom_dir = os.path.join(config.ORIGINAL_SERIES_DIR, series_uid)\n",
    "        if os.path.exists(dicom_dir):\n",
    "            dcm_files = [f for f in os.listdir(dicom_dir) if f.endswith('.dcm') or f.endswith('.DCM')]\n",
    "            if dcm_files:\n",
    "                # Read only the first DICOM file header\n",
    "                ds = pydicom.dcmread(os.path.join(dicom_dir, dcm_files[0]), stop_before_pixels=True, force=True)\n",
    "                study_uid = getattr(ds, 'StudyInstanceUID', None)\n",
    "                patient_id = getattr(ds, 'PatientID', None)\n",
    "                return study_uid or f\"fallback_{series_uid[:32]}\", patient_id\n",
    "    except Exception:\n",
    "        pass\n",
    "    return f\"fallback_{series_uid[:32]}\", f\"fallback_{series_uid[:32]}\"\n",
    "\n",
    "@functools.lru_cache(maxsize=5000)\n",
    "def get_patient_group_cached(series_uid: str) -> str:\n",
    "    \"\"\"Gets a unique ID for grouping patients.\"\"\"\n",
    "    study_uid, patient_id = extract_dicom_patient_info(series_uid)\n",
    "    # Prefer StudyUID if available, as PatientID can be non-unique\n",
    "    return study_uid if study_uid and not study_uid.startswith('fallback_') else patient_id\n",
    "\n",
    "def create_robust_cv_split(train_df_local, n_splits=5):\n",
    "    \"\"\"\n",
    "    This is the original, correct function that reads DICOM headers\n",
    "    to get the PatientID for grouping.\n",
    "    \"\"\"\n",
    "    print(\"Creating patient-separated cross-validation split...\")\n",
    "    patient_groups = []\n",
    "    # This loop is slow, but it is robust and correct.\n",
    "    for series_uid in tqdm(train_df_local['SeriesInstanceUID'], desc=\"Reading Patient Info for CV Split\"):\n",
    "        patient_group = get_patient_group_cached(series_uid)\n",
    "        patient_groups.append(patient_group)\n",
    "        \n",
    "    train_df_local = train_df_local.copy()\n",
    "    train_df_local['patient_id'] = patient_groups\n",
    "    \n",
    "    n_groups = train_df_local['patient_id'].nunique()\n",
    "    print(f\"Total unique patients found: {n_groups}\")\n",
    "    \n",
    "    if n_groups < n_splits:\n",
    "        print(f\"Warning: Only {n_groups} patients. Falling back to StratifiedKFold.\")\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        return list(skf.split(train_df_local, train_df_local['Aneurysm Present']))\n",
    "        \n",
    "    gkf = GroupKFold(n_splits=n_splits)\n",
    "    splits = list(gkf.split(train_df_local, groups=train_df_local['patient_id']))\n",
    "    return splits\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 11. TRAINING: TRAIN/VAL LOOPS\n",
    "# -------------------------\n",
    "def train_epoch_optimized(model, train_loader, criterion, optimizer, scaler, device, accumulation_steps):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for batch_idx, (images, targets, metadata) in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "        metadata = metadata.to(device, non_blocking=True)\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(images, metadata)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss = loss / accumulation_steps\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_loader):\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        running_loss += loss.item() * accumulation_steps\n",
    "        \n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "def validate_epoch_optimized(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_outputs = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets, metadata in tqdm(val_loader, desc=\"Validating\"):\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "            metadata = metadata.to(device, non_blocking=True)\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits = model(images, metadata)\n",
    "                loss = criterion(logits, targets)\n",
    "                \n",
    "            outputs = torch.sigmoid(logits)\n",
    "            running_loss += loss.item()\n",
    "            all_outputs.append(outputs.cpu().numpy())\n",
    "            all_targets.append(targets.cpu().numpy())\n",
    "            \n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    \n",
    "    final_score, aneurysm_auc, avg_individual = calculate_competition_metric(all_targets, all_outputs)\n",
    "    return running_loss / len(val_loader), final_score, aneurysm_auc, avg_individual\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6979cfa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T08:44:57.116181Z",
     "iopub.status.busy": "2025-11-15T08:44:57.115844Z",
     "iopub.status.idle": "2025-11-15T11:28:41.535588Z",
     "shell.execute_reply": "2025-11-15T11:28:41.534594Z"
    },
    "papermill": {
     "duration": 9824.424542,
     "end_time": "2025-11-15T11:28:41.536842",
     "exception": false,
     "start_time": "2025-11-15T08:44:57.112300",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading main CSVs...\n",
      "Creating 8-frame optimized structured paths from PNG dataset...\n",
      "Scanning PNG directory: /kaggle/input/another1/cvt_png\n",
      "Found PNGs for 13 series.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing series: 100%|██████████| 4348/4348 [04:57<00:00, 14.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered train data shape (series with paths): (4348, 18)\n",
      "Creating patient-separated cross-validation split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Patient Info for CV Split: 100%|██████████| 4348/4348 [05:40<00:00, 12.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique patients found: 4348\n",
      "\n",
      "===== STARTING 5-FOLD TRAINING =====\n",
      "\n",
      "--- Fold 0/4 ---\n",
      "Train size: 3478, Val size: 870\n",
      "Train batches: 579, Val batches: 145\n",
      "Loading backbone: tf_efficientnetv2_s.in1k\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35aaa7f42209402f807e3af86d65f505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/86.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone tf_efficientnetv2_s.in1k: 1280 features\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 579/579 [11:43<00:00,  1.22s/it]\n",
      "Validating: 100%|██████████| 145/145 [02:57<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 Epoch 1 - Train Loss: 0.524600, Val Loss: 0.479817, Val Score: 0.575996\n",
      "Aneurysm AUC: 0.6291, Avg Location AUC: 0.5229\n",
      "Saved checkpoint (Best Score): /kaggle/working/eightframe_efficientnetv2s_fold0_epoch1_score0.575996.pth\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 579/579 [09:42<00:00,  1.01s/it]\n",
      "Validating: 100%|██████████| 145/145 [02:22<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 Epoch 2 - Train Loss: 0.396845, Val Loss: 0.412818, Val Score: 0.576621\n",
      "Aneurysm AUC: 0.6246, Avg Location AUC: 0.5286\n",
      "Saved checkpoint (Best Score): /kaggle/working/eightframe_efficientnetv2s_fold0_epoch2_score0.576621.pth\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 579/579 [09:33<00:00,  1.01it/s]\n",
      "Validating: 100%|██████████| 145/145 [02:22<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 Epoch 3 - Train Loss: 0.330528, Val Loss: 0.372256, Val Score: 0.591489\n",
      "Aneurysm AUC: 0.6464, Avg Location AUC: 0.5366\n",
      "Saved checkpoint (Best Score): /kaggle/working/eightframe_efficientnetv2s_fold0_epoch3_score0.591489.pth\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 579/579 [09:22<00:00,  1.03it/s]\n",
      "Validating: 100%|██████████| 145/145 [02:22<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 Epoch 4 - Train Loss: 0.296368, Val Loss: 0.340348, Val Score: 0.587509\n",
      "Aneurysm AUC: 0.6368, Avg Location AUC: 0.5383\n",
      "No improvement. Patience: 1/5\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 579/579 [09:36<00:00,  1.00it/s]\n",
      "Validating: 100%|██████████| 145/145 [02:28<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 Epoch 5 - Train Loss: 0.277381, Val Loss: 0.322904, Val Score: 0.600076\n",
      "Aneurysm AUC: 0.6449, Avg Location AUC: 0.5553\n",
      "Saved checkpoint (Best Score): /kaggle/working/eightframe_efficientnetv2s_fold0_epoch5_score0.600076.pth\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 579/579 [09:44<00:00,  1.01s/it]\n",
      "Validating: 100%|██████████| 145/145 [02:22<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 Epoch 6 - Train Loss: 0.265731, Val Loss: 0.313361, Val Score: 0.591003\n",
      "Aneurysm AUC: 0.6340, Avg Location AUC: 0.5480\n",
      "No improvement. Patience: 1/5\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 579/579 [09:44<00:00,  1.01s/it]\n",
      "Validating: 100%|██████████| 145/145 [02:26<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 Epoch 7 - Train Loss: 0.259753, Val Loss: 0.303888, Val Score: 0.604576\n",
      "Aneurysm AUC: 0.6568, Avg Location AUC: 0.5524\n",
      "Saved checkpoint (Best Score): /kaggle/working/eightframe_efficientnetv2s_fold0_epoch7_score0.604576.pth\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 579/579 [09:40<00:00,  1.00s/it]\n",
      "Validating: 100%|██████████| 145/145 [02:22<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 Epoch 8 - Train Loss: 0.256307, Val Loss: 0.308050, Val Score: 0.603865\n",
      "Aneurysm AUC: 0.6458, Avg Location AUC: 0.5620\n",
      "No improvement. Patience: 1/5\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 579/579 [09:29<00:00,  1.02it/s]\n",
      "Validating: 100%|██████████| 145/145 [02:23<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 Epoch 9 - Train Loss: 0.253773, Val Loss: 0.302236, Val Score: 0.599091\n",
      "Aneurysm AUC: 0.6424, Avg Location AUC: 0.5558\n",
      "No improvement. Patience: 2/5\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 579/579 [09:35<00:00,  1.01it/s]\n",
      "Validating: 100%|██████████| 145/145 [02:22<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 Epoch 10 - Train Loss: 0.253470, Val Loss: 0.301461, Val Score: 0.609154\n",
      "Aneurysm AUC: 0.6526, Avg Location AUC: 0.5658\n",
      "Saved checkpoint (Best Score): /kaggle/working/eightframe_efficientnetv2s_fold0_epoch10_score0.609154.pth\n",
      "\n",
      "Fold 0 finished. Best Score: 0.609154 at epoch 10\n",
      "\n",
      "===== ALL 5 FOLDS TRAINING COMPLETE =====\n",
      "Your models are saved in the '/kaggle/working' directory.\n",
      "Please download them, upload as a new dataset, and create a new notebook for submission.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# -------------------------\n",
    "# 12. TRAINING: MAIN EXECUTION\n",
    "# -------------------------\n",
    "print(\"Loading main CSVs...\")\n",
    "\n",
    "# --- Load all data ---\n",
    "try:\n",
    "    if not os.path.exists(config.TRAIN_CSV_PATH):\n",
    "        raise FileNotFoundError(f\"Original train.csv not found at {config.TRAIN_CSV_PATH}\")\n",
    "    if not os.path.exists(config.SERIES_MAPPING_PATH):\n",
    "         raise FileNotFoundError(f\"series_index_mapping.csv not found at {config.SERIES_MAPPING_PATH}\")\n",
    "         \n",
    "    train_df = pd.read_csv(config.TRAIN_CSV_PATH)\n",
    "    series_mapping_df = pd.read_csv(config.SERIES_MAPPING_PATH)\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ERROR: {e}\")\n",
    "    print(f\"Please make sure your PNG dataset ('another1') and the original competition data are both added to the notebook.\")\n",
    "    # Stop training if files are missing\n",
    "    train_df = None\n",
    "\n",
    "if train_df is not None:\n",
    "    # Build frame paths dict ONCE\n",
    "    frame_paths_dict = create_frame_paths_8frame_structured(train_df, series_mapping_df)\n",
    "    \n",
    "    # Filter train_df to valid series ONCE\n",
    "    valid_series = [uid for uid, entry in frame_paths_dict.items() if len(entry['paths']) > 0]\n",
    "    train_df_filtered = train_df[train_df['SeriesInstanceUID'].isin(valid_series)].copy()\n",
    "    print(f\"Filtered train data shape (series with paths): {train_df_filtered.shape}\")\n",
    "    \n",
    "    # Create CV splits ONCE\n",
    "    cv_splits = create_robust_cv_split(train_df_filtered, config.NUM_FOLDS)\n",
    "    \n",
    "    # --- Train all 5 folds for the ensemble ---\n",
    "    print(f\"\\n===== STARTING {config.NUM_FOLDS}-FOLD TRAINING =====\")\n",
    "    \n",
    "    for fold in range(1):\n",
    "        print(f\"\\n--- Fold {fold}/{config.NUM_FOLDS - 1} ---\")\n",
    "        \n",
    "        # --- 1. Get Fold Data ---\n",
    "        train_indices, val_indices = cv_splits[fold]\n",
    "        train_fold_df = train_df_filtered.iloc[train_indices]\n",
    "        val_fold_df = train_df_filtered.iloc[val_indices]\n",
    "        print(f\"Train size: {len(train_fold_df)}, Val size: {len(val_fold_df)}\")\n",
    "\n",
    "        # --- 2. Create Datasets & Loaders ---\n",
    "        train_dataset = EightFrameDataset(train_fold_df, frame_paths_dict, series_mapping_df, \n",
    "                                          num_frames=config.NUM_FRAMES, transform=train_transform, is_training=True)\n",
    "        val_dataset = EightFrameDataset(val_fold_df, frame_paths_dict, series_mapping_df, \n",
    "                                        num_frames=config.NUM_FRAMES, transform=val_transform, is_training=False)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True, \n",
    "                                  num_workers=config.NUM_WORKERS, pin_memory=config.PIN_MEMORY, \n",
    "                                  drop_last=True, prefetch_factor=config.PREFETCH_FACTOR, \n",
    "                                  persistent_workers=config.PERSISTENT_WORKERS)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False, \n",
    "                                num_workers=config.NUM_WORKERS, pin_memory=config.PIN_MEMORY, \n",
    "                                prefetch_factor=config.PREFETCH_FACTOR, \n",
    "                                persistent_workers=config.PERSISTENT_WORKERS)\n",
    "        print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n",
    "\n",
    "        # --- 3. Initialize Model, Optimizer, etc. (Fresh for each fold) ---\n",
    "        model = ImprovedMultiFrameModel(num_frames=config.NUM_FRAMES, num_classes=config.NUM_CLASSES, pretrained=True)\n",
    "        model = model.to(device)\n",
    "        \n",
    "        criterion = get_loss_function()\n",
    "        optimizer = AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=1e-4)\n",
    "        scheduler = CosineAnnealingLR(optimizer, T_max=config.NUM_EPOCHS, eta_min=1e-6)\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "        # --- 4. Run Training Loop for this fold ---\n",
    "        best_score = 0.0\n",
    "        best_epoch = 0\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(config.NUM_EPOCHS):\n",
    "            print(f\"\\nEpoch {epoch+1}/{config.NUM_EPOCHS}\")\n",
    "            \n",
    "            train_loss = train_epoch_optimized(model, train_loader, criterion, optimizer, scaler, device, config.ACCUMULATION_STEPS)\n",
    "            val_loss, val_score, aneurysm_auc, avg_individual = validate_epoch_optimized(model, val_loader, criterion, device)\n",
    "            scheduler.step()\n",
    "            \n",
    "            print(f\"Fold {fold} Epoch {epoch+1} - Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}, Val Score: {val_score:.6f}\")\n",
    "            print(f\"Aneurysm AUC: {aneurysm_auc:.4f}, Avg Location AUC: {avg_individual:.4f}\")\n",
    "            \n",
    "            if val_score > best_score:\n",
    "                best_score = val_score\n",
    "                best_epoch = epoch + 1\n",
    "                patience_counter = 0\n",
    "                ckpt_path = save_checkpoint(model, optimizer, scheduler, epoch+1, best_score, val_loss, \n",
    "                                            config.OUTPUT_DIR, config.MODEL_NAME, fold)\n",
    "                print(f\"Saved checkpoint (Best Score): {ckpt_path}\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                print(f\"No improvement. Patience: {patience_counter}/{config.EARLY_STOPPING_PATIENCE}\")\n",
    "                if patience_counter >= config.EARLY_STOPPING_PATIENCE:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "        print(f\"\\nFold {fold} finished. Best Score: {best_score:.6f} at epoch {best_epoch}\")\n",
    "        \n",
    "        # Clean up memory before next fold\n",
    "        del model, train_dataset, val_dataset, train_loader, val_loader, optimizer, scheduler, scaler\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"\\n===== ALL 5 FOLDS TRAINING COMPLETE =====\")\n",
    "    print(\"Your models are saved in the '/kaggle/working' directory.\")\n",
    "    print(\"Please download them, upload as a new dataset, and create a new notebook for submission.\")\n",
    "\n",
    "else:\n",
    "    print(\"TRAIN_MODEL is False. Skipping training.\")\n",
    "    print(\"To train your models, set TRAIN_MODEL = True at the top of the script.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d54c5b",
   "metadata": {
    "papermill": {
     "duration": 0.38956,
     "end_time": "2025-11-15T11:28:42.316673",
     "exception": false,
     "start_time": "2025-11-15T11:28:41.927113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95445135",
   "metadata": {
    "papermill": {
     "duration": 0.384696,
     "end_time": "2025-11-15T11:28:43.194284",
     "exception": false,
     "start_time": "2025-11-15T11:28:42.809588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0793d70",
   "metadata": {
    "papermill": {
     "duration": 0.37826,
     "end_time": "2025-11-15T11:28:43.953989",
     "exception": false,
     "start_time": "2025-11-15T11:28:43.575729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13851420,
     "isSourceIdPinned": false,
     "sourceId": 99552,
     "sourceType": "competition"
    },
    {
     "datasetId": 8739150,
     "sourceId": 13735059,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9849.763993,
   "end_time": "2025-11-15T11:28:46.951707",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-15T08:44:37.187714",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "2c039fb431dc4e63b635e0053d3302ac": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2cf64ed0454c41fe80f250df8a23fde9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "35aaa7f42209402f807e3af86d65f505": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f4e9f311339f43e48b21ecdc313809c1",
        "IPY_MODEL_baab3b95a3044a8485e63f8c7f3b7fe9",
        "IPY_MODEL_38fdb2815cfe4703ae1c5bb0e6ef6c5f"
       ],
       "layout": "IPY_MODEL_af5b6c22fa8a4330af3f99af4bea4ac3",
       "tabbable": null,
       "tooltip": null
      }
     },
     "38fdb2815cfe4703ae1c5bb0e6ef6c5f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_95ae863113a54780b2434a3871dbacdf",
       "placeholder": "​",
       "style": "IPY_MODEL_5505b5855a8b413f86180711012c90be",
       "tabbable": null,
       "tooltip": null,
       "value": " 86.5M/86.5M [00:01&lt;00:00, 97.5MB/s]"
      }
     },
     "533e16803e19450797c938a586948426": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5505b5855a8b413f86180711012c90be": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "95ae863113a54780b2434a3871dbacdf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "af5b6c22fa8a4330af3f99af4bea4ac3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "baab3b95a3044a8485e63f8c7f3b7fe9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2c039fb431dc4e63b635e0053d3302ac",
       "max": 86523256.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e8e79b98410c4b51bd21ab69e83ef623",
       "tabbable": null,
       "tooltip": null,
       "value": 86523256.0
      }
     },
     "e8e79b98410c4b51bd21ab69e83ef623": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f4e9f311339f43e48b21ecdc313809c1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2cf64ed0454c41fe80f250df8a23fde9",
       "placeholder": "​",
       "style": "IPY_MODEL_533e16803e19450797c938a586948426",
       "tabbable": null,
       "tooltip": null,
       "value": "model.safetensors: 100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
