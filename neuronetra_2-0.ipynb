{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed22767c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T04:57:32.507605Z",
     "iopub.status.busy": "2025-11-17T04:57:32.507393Z",
     "iopub.status.idle": "2025-11-17T04:58:29.566565Z",
     "shell.execute_reply": "2025-11-17T04:58:29.565750Z"
    },
    "papermill": {
     "duration": 57.063858,
     "end_time": "2025-11-17T04:58:29.567844",
     "exception": false,
     "start_time": "2025-11-17T04:57:32.503986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/albumentations/check_version.py:147: UserWarning: Error fetching version info <urlopen error [Errno -3] Temporary failure in name resolution>\n",
      "  data = fetch_version_info()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful. PyTorch version: 2.6.0+cu124\n",
      "Using device: cuda\n",
      "Loading 3 models from: /kaggle/input/new-models/pytorch/default/1\n",
      "Loading: effnetv2s_fold0_epoch2_score0.587637.pth (Backbone: tf_efficientnetv2_s.in1k)\n",
      "Loading: convnext_tiny_fold0_epoch4_score0.606486.pth (Backbone: convnext_tiny.fb_in1k)\n",
      "Loading: maxvit_tiny_fold0_epoch1_score0.415498.pth (Backbone: maxvit_tiny_tf_224.in1k)\n",
      "\n",
      "Successfully loaded 3 total models.\n",
      "Starting RSNA Inference Server...\n",
      "\n",
      "Script finished.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# PHASE 2: 3-MODEL SUBMISSION SCRIPT (Official Demo Format)\n",
    "# ==========================================\n",
    "# This script loads 3 specific models and uses the\n",
    "# official kaggle_evaluation.rsna_inference_server.\n",
    "\n",
    "# -------------------------\n",
    "# 1. GLOBAL IMPORTS\n",
    "# -------------------------\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import functools\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import typing\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "import gc\n",
    "import shutil\n",
    "\n",
    "# --- Competition-specific import ---\n",
    "# This is the correct library, as shown in the demo\n",
    "import kaggle_evaluation.rsna_inference_server as rsna_inference_server\n",
    "import polars as pl # Demo uses polars for the return type\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import timm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import pydicom\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(f\"Imports successful. PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# -------------------------\n",
    "# 2. GLOBAL CONFIGURATION\n",
    "# -------------------------\n",
    "class Config:\n",
    "    # -----------------------------------------------------------------\n",
    "    # --- !!! 1. EDIT THIS PATH !!! ---\n",
    "    # Change this to your new Kaggle dataset containing the 3 models\n",
    "    MODEL_DIR = \"/kaggle/input/new-models/pytorch/default/1\" \n",
    "    \n",
    "    # --- !!! 2. EDIT THESE FILENAMES !!! ---\n",
    "    # Change these to the exact names of your 3 downloaded files\n",
    "    MODEL_FILES = {\n",
    "        \"effnetv2s\": \"effnetv2s_fold0_epoch2_score0.587637.pth\", # Example\n",
    "        \"convnext_tiny\": \"convnext_tiny_fold0_epoch4_score0.606486.pth\", # Example\n",
    "        \"maxvit_tiny\": \"maxvit_tiny_fold0_epoch1_score0.415498.pth\" # Example\n",
    "    }\n",
    "    # -----------------------------------------------------------------\n",
    "    \n",
    "    # --- Model Hyperparameters (MUST MATCH TRAINING) ---\n",
    "    NUM_FRAMES = 8\n",
    "    IMAGE_SIZE = 224\n",
    "    NUM_CLASSES = 14\n",
    "    \n",
    "    # --- Feature Flags (MUST MATCH TRAINING) ---\n",
    "    USE_METADATA = True\n",
    "    USE_WINDOWING = True\n",
    "    USE_CLAHE = True\n",
    "    \n",
    "    # --- Inference Config ---\n",
    "    TTA_ENABLED = True \n",
    "\n",
    "    # --- Define Models (MUST MATCH TRAINING) ---\n",
    "    MODELS_TO_TRAIN_DICT = {\n",
    "        \"effnetv2s\": \"tf_efficientnetv2_s.in1k\",\n",
    "        \"convnext_tiny\": \"convnext_tiny.fb_in1k\",\n",
    "        \"maxvit_tiny\": \"maxvit_tiny_tf_224.in1k\",\n",
    "    }\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# -------------------------\n",
    "# 3. GLOBAL SEED & DEVICE\n",
    "# -------------------------\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# -------------------------\n",
    "# 4. GLOBAL TARGETS\n",
    "# -------------------------\n",
    "TARGET_COLS = [\n",
    "    'Left Infraclinoid Internal Carotid Artery', 'Right Infraclinoid Internal Carotid Artery', \n",
    "    'Left Supraclinoid Internal Carotid Artery', 'Right Supraclinoid Internal Carotid Artery',\n",
    "    'Left Middle Cerebral Artery', 'Right Middle Cerebral Artery',\n",
    "    'Anterior Communicating Artery', 'Left Anterior Cerebral Artery',\n",
    "    'Right Anterior Cerebral Artery', 'Left Posterior Communicating Artery',\n",
    "    'Right Posterior Communicating Artery', 'Basilar Tip',\n",
    "    'Other Posterior Circulation', 'Aneurysm Present'\n",
    "]\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 5. RE-USED HELPER FUNCTIONS (FROM TRAINING SCRIPT)\n",
    "# ----------------------------------------------------\n",
    "def get_windowing_params(modality: str) -> Tuple[float, float]:\n",
    "    windows = {'CT': (40, 80), 'CTA': (50, 350), 'MRA': (600, 1200), 'MRI': (40, 80), 'MR': (40, 80)}\n",
    "    return windows.get(modality, (40, 80))\n",
    "\n",
    "def apply_dicom_windowing(img: np.ndarray, window_center: float, window_width: float) -> np.ndarray:\n",
    "    img_min = window_center - window_width // 2\n",
    "    img_max = window_center + window_width // 2\n",
    "    img = np.clip(img, img_min, img_max)\n",
    "    img = (img - img_min) / (img_max - img_min + 1e-7)\n",
    "    return (img * 255).astype(np.uint8)\n",
    "\n",
    "def apply_clahe_normalization(img: np.ndarray, modality: str) -> np.ndarray:\n",
    "    if not config.USE_CLAHE: return img.astype(np.uint8)\n",
    "    img = img.astype(np.uint8)\n",
    "    if modality in ['CTA', 'MRA']:\n",
    "        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "        img_clahe = clahe.apply(img)\n",
    "        img_clahe = cv2.convertScaleAbs(img_clahe, alpha=1.1, beta=5)\n",
    "    elif modality in ['MRI', 'MR']:\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        img_clahe = clahe.apply(img)\n",
    "        img_clahe = np.power(img_clahe / 255.0, 0.9) * 255\n",
    "        img_clahe = img_clahe.astype(np.uint8)\n",
    "    else:\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.5, tileGridSize=(8, 8))\n",
    "        img_clahe = clahe.apply(img)\n",
    "    return img_clahe\n",
    "\n",
    "def robust_normalization(volume: np.ndarray) -> np.ndarray:\n",
    "    if volume.size == 0:\n",
    "        return np.zeros((config.IMAGE_SIZE, config.IMAGE_SIZE), dtype=np.uint8)\n",
    "    p1, p99 = np.percentile(volume.flatten(), [1, 99])\n",
    "    volume_norm = np.clip(volume, p1, p99)\n",
    "    if p99 > p1:\n",
    "        volume_norm = (volume_norm - p1) / (p99 - p1 + 1e-7)\n",
    "    else:\n",
    "        volume_norm = np.zeros_like(volume_norm)\n",
    "    return (volume_norm * 255).astype(np.uint8)\n",
    "\n",
    "def create_3channel_input_8frame(volume: np.ndarray) -> np.ndarray:\n",
    "    if len(volume) == 0:\n",
    "        return np.zeros((config.IMAGE_SIZE, config.IMAGE_SIZE, 3), dtype=np.uint8)\n",
    "    middle_slice = volume[len(volume) // 2]\n",
    "    mip = np.max(volume, axis=0)\n",
    "    std_proj = np.std(volume, axis=0).astype(np.float32)\n",
    "    if std_proj.max() > std_proj.min():\n",
    "        p1, p99 = np.percentile(std_proj, [5, 95])\n",
    "        std_proj = np.clip(std_proj, p1, p99)\n",
    "        std_proj = ((std_proj - p1) / (p99 - p1 + 1e-7) * 255).astype(np.uint8)\n",
    "    else:\n",
    "        std_proj = np.zeros_like(std_proj, dtype=np.uint8)\n",
    "    return np.stack([middle_slice, mip, std_proj], axis=-1)\n",
    "\n",
    "def smart_8_frame_sampling(volume_paths: List[str]) -> List[str]:\n",
    "    n = len(volume_paths)\n",
    "    if n == 0: return []\n",
    "    if n <= 8:\n",
    "        result = volume_paths[:]\n",
    "        while len(result) < 8:\n",
    "            result.extend(volume_paths[:8-len(result)])\n",
    "        return result[:8]\n",
    "    start_idx = max(0, int(n * 0.1))\n",
    "    available_frames = n - start_idx\n",
    "    step = max(1, available_frames // 8)\n",
    "    indices = [start_idx + i * step for i in range(8)]\n",
    "    indices = [min(i, n - 1) for i in indices]\n",
    "    if len(set(indices)) < 8:\n",
    "        indices = np.linspace(start_idx, n-1, 8).astype(int).tolist()\n",
    "    return [volume_paths[i] for i in indices]\n",
    "\n",
    "# ---------------------------------\n",
    "# 6. MODEL DEFINITION (FROM TRAINING)\n",
    "# ---------------------------------\n",
    "class ImprovedMultiFrameModel(nn.Module):\n",
    "    def __init__(self, model_name_backbone: str, num_frames=8, num_classes=14, pretrained=False):\n",
    "        super(ImprovedMultiFrameModel, self).__init__()\n",
    "        self.model_name_backbone = model_name_backbone\n",
    "        self.num_frames = num_frames\n",
    "        self.num_classes = num_classes\n",
    "        self.use_metadata = config.USE_METADATA\n",
    "        \n",
    "        self.backbone = timm.create_model(\n",
    "            self.model_name_backbone,\n",
    "            pretrained=pretrained,\n",
    "            num_classes=0,\n",
    "            global_pool='avg'\n",
    "        )\n",
    "        self.feature_dim = self.backbone.num_features\n",
    "        \n",
    "        if self.use_metadata:\n",
    "            self.meta_fc = nn.Sequential(\n",
    "                nn.Linear(2, 16), nn.ReLU(), nn.Dropout(0.2),\n",
    "                nn.Linear(16, 32), nn.ReLU()\n",
    "            )\n",
    "            classifier_input_dim = self.feature_dim + 32\n",
    "        else:\n",
    "            classifier_input_dim = self.feature_dim\n",
    "            \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(classifier_input_dim, 512),\n",
    "            nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, meta=None):\n",
    "        features = self.backbone(x)\n",
    "        if self.use_metadata and meta is not None:\n",
    "            meta_features = self.meta_fc(meta)\n",
    "            features = torch.cat([features, meta_features], dim=1)\n",
    "        output = self.classifier(features)\n",
    "        return output\n",
    "\n",
    "# -------------------------\n",
    "# 7. INFERENCE: TRANSFORMS\n",
    "# -------------------------\n",
    "# We create a dictionary of transforms, one for each model type\n",
    "def get_model_transforms() -> Dict[str, A.Compose]:\n",
    "    transforms = {}\n",
    "    \n",
    "    # PyTorch-native model stats\n",
    "    transforms[\"pytorch\"] = A.Compose([\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    \n",
    "    # TensorFlow-ported model stats\n",
    "    transforms[\"tensorflow\"] = A.Compose([\n",
    "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    return transforms\n",
    "\n",
    "_TRANSFORMS = get_model_transforms()\n",
    "\n",
    "# -------------------------\n",
    "# 8. INFERENCE: MODEL LOADING\n",
    "# -------------------------\n",
    "def load_all_models() -> Dict[str, nn.Module]:\n",
    "    \"\"\"\n",
    "    Loads the 3 specific models defined in Config.MODEL_FILES.\n",
    "    \"\"\"\n",
    "    loaded_models = {}\n",
    "    print(f\"Loading 3 models from: {config.MODEL_DIR}\")\n",
    "    \n",
    "    for prefix, filename in config.MODEL_FILES.items():\n",
    "        backbone_name = config.MODELS_TO_TRAIN_DICT[prefix]\n",
    "        model_path = os.path.join(config.MODEL_DIR, filename)\n",
    "        \n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"FATAL: Model file not found: {model_path}\")\n",
    "            print(f\"Please check the `MODEL_FILES` dict in your Config.\")\n",
    "            return None \n",
    "            \n",
    "        print(f\"Loading: {filename} (Backbone: {backbone_name})\")\n",
    "        \n",
    "        model = ImprovedMultiFrameModel(\n",
    "            model_name_backbone=backbone_name,\n",
    "            pretrained=False \n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            ck = torch.load(model_path, map_location=device, weights_only=False)\n",
    "            model.load_state_dict(ck['model_state_dict'])\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR loading checkpoint: {e}\")\n",
    "            return None\n",
    "        \n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        loaded_models[prefix] = model\n",
    "            \n",
    "    print(f\"\\nSuccessfully loaded {len(loaded_models)} total models.\")\n",
    "    return loaded_models\n",
    "\n",
    "_LOADED_MODELS = load_all_models()\n",
    "\n",
    "# -------------------------\n",
    "# 9. INFERENCE FUNCTION (The \"predict\" function)\n",
    "# -------------------------\n",
    "@torch.no_grad()\n",
    "def predict(series_path: str) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Main inference function called by the server.\n",
    "    Reads DICOMs, preprocesses them to match training, and runs ensemble inference.\n",
    "    \"\"\"\n",
    "    if not _LOADED_MODELS:\n",
    "         # This should not happen if models loaded, but as a safeguard\n",
    "         print(\"ERROR: Models are not loaded. Returning 0.5\")\n",
    "         return pl.DataFrame([tuple([0.5] * len(TARGET_COLS))], schema=TARGET_COLS)\n",
    "\n",
    "    # ------------- 1. Collect & Sample Files -------------\n",
    "    all_filepaths = sorted(glob.glob(os.path.join(series_path, \"*.dcm\")))\n",
    "    if len(all_filepaths) == 0:\n",
    "        # No files found, return default\n",
    "        return pl.DataFrame([tuple([0.5] * len(TARGET_COLS))], schema=TARGET_COLS)\n",
    "    \n",
    "    chosen_paths = smart_8_frame_sampling(all_filepaths)\n",
    "    \n",
    "    # ------------- 2. Preprocess DICOMs (Matching your training) -------------\n",
    "    volume = []\n",
    "    modality = 'CT'\n",
    "    for i, fp in enumerate(chosen_paths):\n",
    "        try:\n",
    "            ds = pydicom.dcmread(fp, force=True)\n",
    "            if i == 0:\n",
    "                modality = getattr(ds, 'Modality', 'CT')\n",
    "                \n",
    "            img = ds.pixel_array.astype(np.float32)\n",
    "            \n",
    "            if hasattr(ds, 'RescaleSlope') and hasattr(ds, 'RescaleIntercept'):\n",
    "                img = img * float(ds.RescaleSlope) + float(ds.RescaleIntercept)\n",
    "            \n",
    "            wc, ww = get_windowing_params(modality)\n",
    "            img = apply_dicom_windowing(img, wc, ww)\n",
    "            img = apply_clahe_normalization(img, modality)\n",
    "            img = cv2.resize(img, (config.IMAGE_SIZE, config.IMAGE_SIZE), interpolation=cv2.INTER_AREA)\n",
    "            volume.append(img)\n",
    "        except Exception:\n",
    "            volume.append(np.zeros((config.IMAGE_SIZE, config.IMAGE_SIZE), dtype=np.uint8))\n",
    "\n",
    "    while len(volume) < 8:\n",
    "        volume.append(np.zeros((config.IMAGE_SIZE, config.IMAGE_SIZE), dtype=np.uint8))\n",
    "        \n",
    "    vol_np = np.array(volume)\n",
    "    \n",
    "    # ------------- 3. Create 3-Channel Input (matching training) -------------\n",
    "    vol_norm = robust_normalization(vol_np)\n",
    "    input_hwc = create_3channel_input_8frame(vol_norm)\n",
    "    \n",
    "    # ------------- 4. Get Metadata Placeholder -------------\n",
    "    # We don't have real metadata, so we use a neutral placeholder (0.5 for 50yrs, 0.5 for ambiguous sex)\n",
    "    # This MUST match what your model expects (a batch of 1)\n",
    "    meta_tensor = torch.tensor([[0.5, 0.5]], dtype=torch.float32).to(device)\n",
    "\n",
    "    # ------------- 5. Model Inference (Ensemble Average) -------------\n",
    "    all_model_probs = []\n",
    "    \n",
    "    for prefix, model in _LOADED_MODELS.items():\n",
    "        backbone_name = config.MODELS_TO_TRAIN_DICT[prefix]\n",
    "        \n",
    "        # Get the correct transform for this model\n",
    "        if 'tf_' in backbone_name:\n",
    "            transform = _TRANSFORMS[\"tensorflow\"]\n",
    "        else:\n",
    "            transform = _TRANSFORMS[\"pytorch\"]\n",
    "            \n",
    "        # Apply transform\n",
    "        img_tensor = transform(image=input_hwc)['image'].unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Standard pred\n",
    "            logits = model(img_tensor, meta_tensor)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            \n",
    "            # TTA (Horizontal Flip)\n",
    "            if config.TTA_ENABLED:\n",
    "                img_tensor_flipped = torch.flip(img_tensor, dims=[-1])\n",
    "                logits_f = model(img_tensor_flipped, meta_tensor)\n",
    "                probs_f = torch.sigmoid(logits_f)\n",
    "                # Average TTA\n",
    "                probs = (probs + probs_f) / 2.0\n",
    "        \n",
    "        all_model_probs.append(probs)\n",
    "\n",
    "    # Average ensemble predictions\n",
    "    final_probs = torch.mean(torch.stack(all_model_probs, dim=0), dim=0)\n",
    "    \n",
    "    # Format for polars DataFrame\n",
    "    row = final_probs.cpu().numpy()[0].tolist()\n",
    "    predictions = pl.DataFrame([tuple(row)], schema=TARGET_COLS)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# --------------------------\n",
    "# 10. START SERVER\n",
    "# --------------------------\n",
    "print(\"Starting RSNA Inference Server...\")\n",
    "# This server will call your `predict()` function for each test series\n",
    "inference_server = rsna_inference_server.RSNAInferenceServer(predict)\n",
    "inference_server.serve()\n",
    "\n",
    "# --- Clean up memory after server is done (if it ever exits) ---\n",
    "if '_LOADED_MODELS' in locals() or '_LOADED_MODELS' in globals():\n",
    "    del _LOADED_MODELS\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nScript finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85814d1b",
   "metadata": {
    "papermill": {
     "duration": 0.001668,
     "end_time": "2025-11-17T04:58:29.571692",
     "exception": false,
     "start_time": "2025-11-17T04:58:29.570024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e218810",
   "metadata": {
    "papermill": {
     "duration": 0.001608,
     "end_time": "2025-11-17T04:58:29.574976",
     "exception": false,
     "start_time": "2025-11-17T04:58:29.573368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cddba9b",
   "metadata": {
    "papermill": {
     "duration": 0.00158,
     "end_time": "2025-11-17T04:58:29.578169",
     "exception": false,
     "start_time": "2025-11-17T04:58:29.576589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0721342",
   "metadata": {
    "papermill": {
     "duration": 0.001569,
     "end_time": "2025-11-17T04:58:29.581417",
     "exception": false,
     "start_time": "2025-11-17T04:58:29.579848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13851420,
     "sourceId": 99552,
     "sourceType": "competition"
    },
    {
     "modelId": 504093,
     "modelInstanceId": 488673,
     "sourceId": 647878,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 62.835982,
   "end_time": "2025-11-17T04:58:32.015390",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-17T04:57:29.179408",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
